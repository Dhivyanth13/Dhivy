import speech_recognition as sr
import pyttsx3
import librosa
import numpy as np
from tensorflow.keras.models import load_model

# Load the pre-trained emotion detection model
emotion_model = load_model('path_to_your_emotion_model.h5')

# Initialize the speech engine for text-to-speech
engine = pyttsx3.init()

# Set properties for voice response
engine.setProperty('rate', 150)  # Speed of speech
engine.setProperty('volume', 1)  # Volume level (0.0 to 1.0)

# Function to extract features from voice data
def extract_features(audio_file):
    audio, sample_rate = librosa.load(audio_file, res_type='kaiser_fast')
    mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40).T, axis=0)
    return np.expand_dims(mfccs, axis=0)

# Function to detect emotion from the voice input
def detect_emotion(audio_file):
    features = extract_features(audio_file)
    emotion_prediction = emotion_model.predict(features)
    emotion_index = np.argmax(emotion_prediction)
    emotion_labels = ['neutral', 'happy', 'sad', 'angry', 'fearful']
    return emotion_labels[emotion_index]

# Function to generate a response based on detected emotion
def generate_emotion_response(emotion):
    if emotion == 'happy':
        return "I'm glad you're feeling happy today!"
    elif emotion == 'sad':
        return "I'm sorry you're feeling sad. I hope things get better."
    elif emotion == 'angry':
        return "It sounds like you're upset. Take a deep breath, and let's talk."
    elif emotion == 'fearful':
        return "It seems like you're anxious. I'm here for you."
    else:
        return "It's nice to chat with you. How can I help?"

# Main function for capturing speech and generating voice responses
def main():
    recognizer = sr.Recognizer()
    mic = sr.Microphone()

    while True:
        try:
            # Listen for voice input
            print("Listening...")
            with mic as source:
                recognizer.adjust_for_ambient_noise(source)
                audio = recognizer.listen(source)

            # Recognize speech using Google Web Speech API
            print("Recognizing speech...")
            text = recognizer.recognize_google(audio)
            print(f"You said: {text}")

            # Save the audio file for emotion detection (You can replace this with live audio processing)
            audio_file = "temp_audio.wav"
            with open(audio_file, "wb") as f:
                f.write(audio.get_wav_data())

            # Detect the emotion from the audio
            detected_emotion = detect_emotion(audio_file)
            print(f"Detected emotion: {detected_emotion}")

            # Generate a response based on the detected emotion
            response = generate_emotion_response(detected_emotion)
            print(f"Response: {response}")

            # Speak out the response
            engine.say(response)
            engine.runAndWait()

        except sr.UnknownValueError:
            print("Sorry, I couldn't understand what you said.")
            engine.say("Sorry, I couldn't understand what you said. Can you repeat?")
            engine.runAndWait()

        except sr.RequestError as e:
            print(f"Could not request results from Google Speech Recognition service; {e}")
            engine.say("There seems to be an issue with the speech recognition service.")
            engine.runAndWait()

if __name__ == '__main__':
    main()
